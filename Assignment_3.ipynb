{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LongC24/UVM_CS254_HW/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7eGcIIaTqs"
      },
      "source": [
        "# Assignment 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTsu2b6AaTqw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
        "from past.builtins import xrange\n",
        "from scipy.special import expit\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRtl3RegaTqy"
      },
      "source": [
        "# Part 1(Logistic regression without regularization)\n",
        "\n",
        "In this exercise, you will implement logistic regression and apply it to the Iris dataset found under data directory.\n",
        "\n",
        "In the Iris dataset there are two continuous variables named \"PetalLengthCm\" and \"PetalWidthCm\". The type of flower associated with these features is assigned in the \"species\" column. Setosa is labeled as 0 and Versicolor is labeled as 1. Your task will be to classify between these two types of flowers using logistic regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "8IAXMkHKaTqz"
      },
      "source": [
        "### Part 1, Linear Logistic Regression \n",
        "\n",
        "1. [5 points] If you have Setosa labeled as 0 and Versicolor is labeled as 1, what is the logistic regression output mean (in words) for a given sample?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pluowSnsaTq0"
      },
      "outputs": [],
      "source": [
        "# add your answer here\n",
        "# The output of 1 os mean its the Versicolor and the out put of the o means it's the \n",
        "# Setosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoaRHeIzaTq0"
      },
      "source": [
        "2. [5 points] Load the data and split it into X(feature vectors) and Y(target/output vectors)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tdVN6WdnanX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTfahdY6aTq1"
      },
      "outputs": [],
      "source": [
        "#  Load Iris dataset\n",
        "# This dataset can be found in the \"data\" folder\n",
        "# note that the data has a header and you don't need to specify the header information when you load it.\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab_Notebooks/Assignment3/data/Iris.csv'\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "#display part of the dataset. \n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phV7DKriaTq3"
      },
      "source": [
        "3. [5 points] Load the data and split it into X(feature vectors) and Y(target/output vector). Add columon of \"ones\" to X and convert both of them to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThJ2R9kXaTq4"
      },
      "outputs": [],
      "source": [
        "data[\"ones\"] = np.ones(data.shape[0])\n",
        "#print(data[\"Id\"])\n",
        "# seperate data into X, Y and convert them to numpy arrays.\n",
        "# you might need to reshape Y to convert it to a matrix of (100, 1) diminsions. \n",
        "X = data[[\"ones\",\"PetalLengthCm\",\"PetalWidthCm\"]].values\n",
        "Y = data[\"Species\"].values.reshape(100, 1)\n",
        "display(data.head())\n",
        "\n",
        "print(\"X shape =\", X.shape,\", Y shape =\", Y.shape, \"X type =\", type(X), \"Y type =\", type(Y))\n",
        "#results should be \"X shape = (100, 3) , Y shape = (100, 1) \n",
        "#X type = <class 'numpy.ndarray'> Y type = <class 'numpy.ndarray'>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZA7cSm8aTq4"
      },
      "source": [
        "This code will visualize your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrxsnJ7paTq5"
      },
      "outputs": [],
      "source": [
        "# access class-based data\n",
        "setosa = data.loc[data['Species'] == 0]\n",
        "versicolor = data.loc[data['Species'] == 1]\n",
        "\n",
        "# data plotting and specifications\n",
        "plt.scatter(setosa['PetalLengthCm'], setosa['PetalWidthCm'], marker = \"+\")\n",
        "plt.scatter(versicolor['PetalLengthCm'], versicolor['PetalWidthCm'], marker = \"o\")\n",
        "\n",
        "# labeling specification\n",
        "plt.xlabel('PetalLengthCm')\n",
        "plt.ylabel('PetalWidthCm')\n",
        "\n",
        "# legend and show calls\n",
        "plt.legend([\"setosa\", \"versicolor\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRIuIl-iaTq6"
      },
      "source": [
        "4. [5 points] Comment on the data. Is it linearly separable? What are the dimensions of the hypothesis hyperplane for this data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHpOVw3_aTq6"
      },
      "outputs": [],
      "source": [
        "#add your answer here\n",
        "# It's Linearly separable. This is one-dimensional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GAiEX_daTq7"
      },
      "outputs": [],
      "source": [
        "def hypothesis(X, theta):\n",
        "    #print(X)\n",
        "    #print(theta)\n",
        "    h =  1/(1+np.exp((np.dot(-X,theta))))\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIE7hAJpaTq7"
      },
      "source": [
        "5. [10 points] Before you start with the logistic regression implementation, you need to first implement the hypothesis function, then the cost function and the gradient descent algorithm. Test it when you are finished. Try testing a few values by calling hypothesis(x). For large positive values of x, the hypothesis should be close to 1, while for large negative values, the hypothesis should be close to 0. Evaluating hypothesis(0) should give you exactly 0.5.\n",
        "The equation for sigma is \n",
        "![H1.png](attachment:H1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERMfO6MbaTq7"
      },
      "outputs": [],
      "source": [
        "# this is a test function for the hypothesis function implemented above.\n",
        "theta = np.zeros((X.shape[1], 1))\n",
        "h = hypothesis(X, theta)\n",
        "print (h.shape) # this should be (100, 1)\n",
        "print (h[0]) # this should be 0.5 for all values in h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU3ZjqxwaTq8"
      },
      "source": [
        "6.  [10 points] Implement a logistic regression cost function (calcLogRegressionCost).\n",
        "\n",
        "The vectorized equation for the cost function is given below for your convenience.  \n",
        "\n",
        "![J1.png](attachment:J1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvd5_8NYaTq9"
      },
      "outputs": [],
      "source": [
        "def calcLogRegressionCost(X, Y, theta_1):\n",
        "    \"\"\"\n",
        "    Calculate Logistic Regression Cost\n",
        "    \n",
        "    X: Features matrix\n",
        "    Y: Output matrix\n",
        "    theta: matrix of variable weights\n",
        "    output: return the cost value.\n",
        "    \"\"\"\n",
        "    h_1 = hypothesis(X, theta_1)\n",
        "\n",
        "    \n",
        "\n",
        "    m_1 = len(X)\n",
        "    #print(m_1)\n",
        "    part_1 = -np.dot(Y.T, np.log(h_1))\n",
        "    part_2 = np.dot((1-Y).T , np.log(1-h_1))\n",
        "    \n",
        "    cost = (part_1 - part_2)/m_1\n",
        "    \n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXvXmwd8aTq9"
      },
      "outputs": [],
      "source": [
        "#This is a test function that will call \"calcLogRegressionCost\" using the theta initial parameters (zeros). \n",
        "# You should get about 0.693.\n",
        "theta = np.zeros((X.shape[1], 1))\n",
        "print(calcLogRegressionCost(X, Y, theta))\n",
        "\n",
        "# Desired output: 0.693..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD3CmesiaTq-"
      },
      "source": [
        "7.  [5 points] Is there a difference between the gradient descent algorithm for linear regression and logistic regression? If yes, what is the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J21jyQmjaTq-"
      },
      "outputs": [],
      "source": [
        "#insert your answer here\n",
        "#the difference between inear regression and logistic regression is the\n",
        "# Logistic regression need add the cost function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi9bIGuJaTq-"
      },
      "source": [
        "8.  [15 points] Implement a logistic Regression Gradient Descent algorithm (logRegressionGradientDescent).\n",
        "\n",
        "The vectorized equation for the logisitc Regression Gradient Descent algorithm is given below for your convenience.\n",
        "![gd.png](attachment:gd.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXrOKD1ZaTq-"
      },
      "outputs": [],
      "source": [
        "def logRegressionGradientDescent(X, Y, theta, eta, iters): \n",
        "\n",
        "    \"\"\"\n",
        "    Performs gradient descent optimization on a set of data\n",
        "    \n",
        "    X: Features matrix\n",
        "    Y: Output matrix\n",
        "    theta: matrix of variable weights\n",
        "    eta: learning rate\n",
        "    iters: number of times to iterate the algorithm (epochs)\n",
        "    output: return optimized theta and the cost array for each iteration (epoch). \n",
        "    \"\"\"\n",
        "    #print(len(X))\n",
        "    cost = np.zeros(iters)\n",
        "    \n",
        "    m = len(X)\n",
        "\n",
        "    for i in range(iters):\n",
        "        h_2 = hypothesis(X,theta)\n",
        "        gradients = 2* (np.dot(X.T, h_2 - Y))/m\n",
        "        theta = theta - eta * gradients\n",
        "        cost[i] = calcLogRegressionCost(X, Y, theta)\n",
        "        #print(cost[i])\n",
        "        #print(gradients)\n",
        "    return theta, cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIlqG4IzaTq_"
      },
      "source": [
        "9. [5 Points] As you have the gradient decent algorithm implemented, run the gradient descent algorithm to fit your parameters theta to the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y7eXT5naTq_"
      },
      "outputs": [],
      "source": [
        "# this is a test function for logRegressionGradientDescent function. You can change eta and iters \n",
        "# but you will not get the values given below.\n",
        "eta = 0.1\n",
        "iters = 10000\n",
        "theta = np.zeros((X.shape[1],1))\n",
        "theta, cost = logRegressionGradientDescent(X, Y, theta, eta, iters)\n",
        "print(calcLogRegressionCost(X, Y, theta))\n",
        "print(theta)\n",
        "# you should get these values\n",
        "# For cost [[0.00358296]]\n",
        "# For theta [[-11.73342243]\n",
        "#  [  3.24410413]\n",
        "#  [  4.8722057 ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0EIElRcaTq_"
      },
      "source": [
        "\n",
        "10. [5 Points] Plot the cost by the number of epochs. Is it as expected, why or why not ? What is the best learning rate you chose, and why ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4EZmWoFaTrA"
      },
      "outputs": [],
      "source": [
        "# write your code here ... \n",
        "for (e, c) in zip(range(iters), cost):\n",
        "    plt.scatter(e,c, color='b')\n",
        "    plt.plot(e, c, \"r\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYc2P-BOaTrA"
      },
      "source": [
        "11. [5 Points] Suppose that the petal length is 4.5 and petal width is 1.5. Using the hypothesis function, find the probability of this sample being a versicolor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_z6TSAbaTrA"
      },
      "outputs": [],
      "source": [
        "x_new = np.array([[1, 4.5, 1.5]])\n",
        "# write your code here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnsxqZ0KaTrA"
      },
      "source": [
        "This code will plot the decision boundary with respect to the data. The plot will include PetalLengthCm in the X-axis and PetalWidthCm in the Y-axis. Additionally, assign '+' for Setosa points and 'o' for versicolor points. The decision boundary separates the classes using the optimal theta found in the answer in the previous questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AG41yKnaTrA"
      },
      "outputs": [],
      "source": [
        "def plotData(feature1, feature2, label1, label2, feature1AxisLabel, feature2AxisLabel):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(feature1[:,1], feature1[:,2], 'ko', label=label1)\n",
        "    plt.plot(feature2[:,1], feature2[:,2], 'r+', label=label2)\n",
        "    plt.xlabel(feature1AxisLabel)\n",
        "    plt.ylabel(feature2AxisLabel)\n",
        "    plt.legend()\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQjepT7zaTrB"
      },
      "outputs": [],
      "source": [
        "def plotDecisionBoundary(X, theta):\n",
        "    \n",
        "    # this will find min,max x values and solve for y = 0 at those positions\n",
        "    boundary_xs = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
        "    boundary_ys = -1 * (-.5 + theta[0] + theta[1]*boundary_xs) / theta[2]\n",
        "    \n",
        "    # plot points\n",
        "    plt.plot(boundary_xs, boundary_ys, 'b-', label='Decision Boundary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HstXtGpEaTrB"
      },
      "outputs": [],
      "source": [
        "pos = np.array([X[i] for i in xrange(X.shape[0]) if Y[i] == 1])\n",
        "neg = np.array([X[i] for i in xrange(X.shape[0]) if Y[i] == 0])\n",
        "\n",
        "plotData(pos, neg, \"versicolor\", \"setosa\", 'PetalLengthCm', 'PetalWidthCm')\n",
        "plotDecisionBoundary(X, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAX56X-laTrB"
      },
      "source": [
        "12. [5 Points] Is what you see in the previous figure is what you expected to see? Justify your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdIrtQibaTrB"
      },
      "outputs": [],
      "source": [
        "# add your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWVqJBssaTrC"
      },
      "source": [
        "# Assignment 3 - Part 2 (Nonlinear and Regularized Logistic regression)\n",
        "\n",
        "In this part, you will be asked to implement a Nonlinear and Regularized Logistic regression using Ridge Regularization. To do this you need to work on the ex2data2.txt dataset found under the data directory. In this dataset, there are two continuous independent variables - “Test 1” and “Test 2”. Our target variable is binary and labeled 0(did not passed the test) or 1(passed the test).\n",
        "\n",
        "In this part of the assignment, you will build a logistic regression model to predict whether a sample passed the test or not (a model that estimates the probability of passing based on test 1 and test 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIoS_bx1aTrC"
      },
      "source": [
        "1. [5 Points] Load the data and split it into X(feature vectors) and Y(target/output vectors). Add a column of \"ones\" to X and convert both of them to numpy arrays. (You can follow the steps from part1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJRS0-p-aTrC"
      },
      "outputs": [],
      "source": [
        "# add your code here ...\n",
        "#display(data.head()\n",
        "dir_path = '/content/drive/MyDrive/Colab_Notebooks/Assignment3/data/ex2data2.txt'\n",
        "data = pd.read_csv(dir_path,header=None)\n",
        "data.columns=[\"Test1\",\"Test2\",\"state\"]\n",
        "#display(data.head())\n",
        "data[\"ones\"] = np.ones(len(data))\n",
        "#display(data.head())\n",
        "\n",
        "X = data[[\"ones\",\"Test1\",\"Test2\"]].values\n",
        "Y = data[\"state\"].values.reshape(118, 1)\n",
        "\n",
        "print(\"X shape =\", X.shape,\", Y shape =\", Y.shape, \"X type =\", type(X), \"Y type =\", type(Y))\n",
        "#display part of the dataset. \n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-G23R_1aTrC"
      },
      "outputs": [],
      "source": [
        "# this code will visualize your data, test 1 will be in the X-axis,\n",
        "# test 2 will be on the Y-axis.\n",
        "# we assigned '+' for pass and 'o' for fail\n",
        "# your asnwer should look the figure below\n",
        "pos = np.array([X[i] for i in xrange(X.shape[0]) if Y[i] == 1])\n",
        "neg = np.array([X[i] for i in xrange(X.shape[0]) if Y[i] == 0])\n",
        "\n",
        "def plotData(pos, neg):\n",
        "    \n",
        "    plt.plot(pos[:,1], pos[:,2], 'k+', label='y=1')\n",
        "    plt.plot(neg[:,1], neg[:,2], 'yo', label='y=0')\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "plotData(pos, neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-eZknNtaTrD"
      },
      "source": [
        "2. [5 Points] As you might have noticed, this data is non-linearly separable. Therefore, we have to transform our features with PolynomialFeatures from sklearn. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr4lOFYSaTrD"
      },
      "source": [
        "You will need to create a PolynomialFeatures object <b>(with the degree hyperparam set to 6)</b> which we will denote as the variable \"poly\". Then you need to transform the data using this object that you have created. Use the docs to assist you with understanding and implementing your code https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
        "Note: please note that you are going to apply polynomials only on the original features without the added column of ones, as the sklearn PolynomialFeatures automatically adds a \"ones\" column to the output transformed X (No need to add \"ones\" after you transform using sklearn PolynomialFeatures)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX_FMDm_aTrD"
      },
      "outputs": [],
      "source": [
        "#poly = add your code here\n",
        "#X_poly = add your code here \n",
        "poly = PolynomialFeatures(6)\n",
        "X_poly = poly.fit_transform(data[[\"Test1\",\"Test2\"]].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIa_kSJwaTrD"
      },
      "outputs": [],
      "source": [
        "#This part is given for testing purpose, you can change eta and iters to different values\n",
        "# but you are not going to have same cost[0] reported below. \n",
        "eta = 0.5\n",
        "iters = 10000\n",
        "\n",
        "theta_poly_init = np.zeros((X_poly.shape[1], 1))  \n",
        "theta_poly, cost = logRegressionGradientDescent(X_poly, Y, theta_poly_init, eta, iters)\n",
        "print (cost[0]) # cost [0] should be 0.6812373150879889"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsN-IA5haTrE"
      },
      "source": [
        "3. [5 Points] Plot the cost vs iterations. Comment on the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLbV4FidaTrE"
      },
      "outputs": [],
      "source": [
        "# add your code here\n",
        "for (e, c) in zip(range(iters), cost):\n",
        "    plt.scatter(e,c, color='b')\n",
        "    plt.plot(e, c, \"r\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3XzVg0KaTrE"
      },
      "source": [
        "This code will plot the decision boundary of the polynominal features. Use logRegressionGradientDescent for plotting the decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odsbu5qNaTrE"
      },
      "outputs": [],
      "source": [
        "def plotBoundary(theta, X, Y, poly, eta, iters):\n",
        "\n",
        "    # find optimal thetas\n",
        "    theta, cost = logRegressionGradientDescent(X, Y, theta, eta, iters)\n",
        "    \n",
        "    # create search space and placeholder\n",
        "    xvals = np.linspace(-1, 1.5, 100)\n",
        "    yvals = np.linspace(-1, 1.5, 100)\n",
        "    zvals = np.zeros((len(xvals), len(yvals)))\n",
        "    \n",
        "    # compute zval for all combinations of xvals/yvals\n",
        "    for i in range(len(xvals)):\n",
        "        for j in range(len(yvals)):\n",
        "            featuresij = poly.fit_transform(np.array([[xvals[i], yvals[j]]]))\n",
        "            zvals[j][i] = np.dot(theta.T, featuresij.T)\n",
        "    \n",
        "    \n",
        "    contour = plt.contour(xvals, yvals, zvals, [0])\n",
        "    plt.title(\"Decision Boundary \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxjbMjnuaTrF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plotData(pos, neg)\n",
        "plotBoundary(theta_poly, X_poly,Y,poly, eta, iters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F429n7DnaTrF"
      },
      "source": [
        "  4. [5 Points] Comment on the decision boundary. Is the decision boundary behaving as expected? Try higher and lower polynomial degrees and comment on the differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIKt3hQKaTrF"
      },
      "source": [
        "### Part 3, For Graduate Students , Extra Cridet for Undergraduates -  [30 pts]\n",
        "\n",
        " Regularization is an important way to fit the data more accurately. Before going further, you need to implement the Ridge regularized gradient descent for logistic regression with its corresponding regularized cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2upsuJnYaTrF"
      },
      "source": [
        "\n",
        "\n",
        "1. [10 Points] Implement the Ridge Regularized Logistic Regression cost function named calcLogRegressionCostR following the given equation.\n",
        "![cost_reg.png](attachment:cost_reg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FmVWHfFaTrG"
      },
      "outputs": [],
      "source": [
        "def calcLogRegressionCostR(X, Y, theta, alpha):\n",
        "    # X : feature vector\n",
        "    # Y : target vector/ output vector\n",
        "    # theta : weight vector\n",
        "    # alpha : regularization parameter\n",
        "    \n",
        "    # add your code here ...\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyEBL3gdaTrG"
      },
      "source": [
        "2. [10 Points] Implement logRegressionGradientDescentR . This should be ridge regularized. The equation is \n",
        "![](images/G3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IiwyG58aTrG"
      },
      "outputs": [],
      "source": [
        "def logRegressionGradientDescentR(X, Y, theta, eta, iters, alpha):\n",
        "    \"\"\"\n",
        "    Caculating gradients and updating thetas through gradient descent loop with regularization\n",
        "    \n",
        "    X: Features matrix\n",
        "    Y: Output matrix\n",
        "    theta: matrix of variable weights\n",
        "    eta: learning rate\n",
        "    iters: number of epochs for updating theta\n",
        "    alpha: regularization term\n",
        "    return optimized theta and cost\n",
        "    \"\"\"\n",
        "    # add your code here \n",
        "   \n",
        "    return theta, cost   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4uH6WYFaTrG"
      },
      "outputs": [],
      "source": [
        "def plotBoundaryR(theta, X, Y, poly, eta, iters, alpha):\n",
        "\n",
        "    # find optimal thetas\n",
        "    theta, cost = logRegressionGradientDescentR(X, Y, theta, eta, iters, alpha)\n",
        "    \n",
        "    # create search space and placeholder\n",
        "    xvals = np.linspace(-1, 1.5, 100)\n",
        "    yvals = np.linspace(-1, 1.5, 100)\n",
        "    zvals = np.zeros((len(xvals), len(yvals)))\n",
        "    \n",
        "    # compute zval for all combinations of xvals/yvals\n",
        "    for i in range(len(xvals)):\n",
        "        for j in range(len(yvals)):\n",
        "            featuresij = poly.fit_transform(np.array([[xvals[i], yvals[j]]]))\n",
        "            zvals[j][i] = np.dot(theta.T, featuresij.T)\n",
        "    \n",
        "    \n",
        "    contour = plt.contour(xvals, yvals, zvals, [0])\n",
        "    plt.title(\"Decision Boundary with Alpha = \" + str(alpha))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A34jEo-aTrG"
      },
      "outputs": [],
      "source": [
        "#This code will visualize the boundaries for different values of alpha.\n",
        "\n",
        "alpha_vals = [0, 0.03, 0.1, 0.5]\n",
        "plt.figure(figsize=(22,15))\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(221 + i)\n",
        "    plotData(pos, neg)\n",
        "    plotBoundaryR(theta_poly, X_poly, Y, poly, eta, iters, alpha=alpha_vals[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ1rI1yiaTrH"
      },
      "source": [
        "3. [10 Points] Comment on the figures above. What is the best combination of polynomial and regularization that works the best, and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxTWHd7haTrH"
      },
      "outputs": [],
      "source": [
        "# add your answer here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}